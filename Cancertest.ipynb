{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "#mnist = input_data.read_data_sets(\"/tmp/data2\",one_hot=\"TRUE\")\n",
    "#from glob import glob\n",
    "import os\n",
    "import glob\n",
    "#glob.glob('./[0-9].*')\n",
    "def read_data_sets(train_dir,\n",
    "                   fake_data=False,\n",
    "                   one_hot=False,\n",
    "                   dtype=dtypes.float32,\n",
    "                   reshape=True,\n",
    "                   validation_size=5000):\n",
    "  if fake_data:\n",
    "\n",
    "    def fake():\n",
    "      return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype)\n",
    "\n",
    "    train = fake()\n",
    "    validation = fake()\n",
    "    test = fake()\n",
    "    return base.Datasets(train=train, validation=validation, test=test)\n",
    "\n",
    "  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "  local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n",
    "                                   SOURCE_URL + TRAIN_IMAGES)\n",
    "  with open(local_file, 'rb') as f:\n",
    "    train_images = extract_images(f)\n",
    "\n",
    "  local_file = base.maybe_download(TRAIN_LABELS, train_dir,\n",
    "                                   SOURCE_URL + TRAIN_LABELS)\n",
    "  with open(local_file, 'rb') as f:\n",
    "    train_labels = extract_labels(f, one_hot=one_hot)\n",
    "\n",
    "  local_file = base.maybe_download(TEST_IMAGES, train_dir,\n",
    "                                   SOURCE_URL + TEST_IMAGES)\n",
    "  with open(local_file, 'rb') as f:\n",
    "    test_images = extract_images(f)\n",
    "\n",
    "  local_file = base.maybe_download(TEST_LABELS, train_dir,\n",
    "                                   SOURCE_URL + TEST_LABELS)\n",
    "  with open(local_file, 'rb') as f:\n",
    "    test_labels = extract_labels(f, one_hot=one_hot)\n",
    "\n",
    "  if not 0 <= validation_size <= len(train_images):\n",
    "    raise ValueError(\n",
    "        'Validation size should be between 0 and {}. Received: {}.'\n",
    "        .format(len(train_images), validation_size))\n",
    "\n",
    "  validation_images = train_images[:validation_size]\n",
    "\n",
    "  validation_labels = train_labels[:validation_size]\n",
    "  train_images = train_images[validation_size:]\n",
    "  train_labels = train_labels[validation_size:]\n",
    "\n",
    "  train = DataSet(train_images, train_labels, dtype=dtype, reshape=reshape)\n",
    "  validation = DataSet(validation_images,\n",
    "                       validation_labels,\n",
    "                       dtype=dtype,\n",
    "                       reshape=reshape)\n",
    "  test = DataSet(test_images, test_labels, dtype=dtype, reshape=reshape)\n",
    "\n",
    "  return base.Datasets(train=train, validation=validation, test=test)\n",
    "\n",
    "\n",
    "\n",
    "type_1_files = glob.glob(os.path.join(\"/tmp/data2/Cervix\",\"*.jpg\"))\n",
    "print len(type_1_files)\n",
    "#import cv2\n",
    "#cv2.imshow(type_1_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Set up the computation graph in tensor flow by\n",
    "    constructing a deep neural network with 3 hidden layers\n",
    "    Feed-forward network: (Forward pass)\n",
    "        input (x) --> hidden layer1 --> hidden layer2 --> hidden layer3 --> output\n",
    "    Backpropagation:\n",
    "        compare model's predicted output with expected output(y) -> cost function (cross entropy)\n",
    "        Optimization function (optimizer) -> minimizes cost (AdamOptimizer, AdaGrad)\n",
    "        Backward pass --> To minimize cost, weights of hidden layers are manipulated\n",
    "        in the backward pass        \n",
    "    Batch size: \n",
    "        Number of training examples in one forward/backward pass, it takes memory\n",
    "    Epoch:\n",
    "        Forward + backward pass of all the training examples.\n",
    "    Iteration:\n",
    "        one pass = forward pass + backward pass\n",
    "        one iteration = number of passes, each pass using [batch size] training examples.\n",
    "\"\"\"\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "n_classes = 10\n",
    "batch_size = 100 \n",
    "num_epochs = 10\n",
    "input_data_size = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" place holder for input and output values \"\"\"\n",
    "x = tf.placeholder('float',[None,input_data_size])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Model the neural network\"\"\"\n",
    "def neural_network_model(data):\n",
    "    \n",
    "    \"\"\"output = (input_data * weights) + biases\n",
    "\t\n",
    "\tW has a shape of [784, 10] because we want\n",
    "\tto multiply the 784-dimensional image vectors by it \n",
    "\tto produce 10-dimensional vectors of evidence for the difference classes.\n",
    "\t\n",
    "\tb has a shape of [10] so we can add it to the output.\"\"\"\n",
    "    \n",
    "    hidden_layer_1 = {\"weights\":tf.Variable(tf.random_normal([input_data_size,n_nodes_hl1])),\n",
    "                      \"biases\":tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    hidden_layer_2 = {\"weights\":tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),\n",
    "                      \"biases\":tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    hidden_layer_3 = {\"weights\":tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),\n",
    "                      \"biases\":tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    output_layer = {\"weights\":tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),\n",
    "                      \"biases\":tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    \"\"\" For each neuron in hidden layer:\n",
    "        output = activation(sum((input*weight)) + biases)\n",
    "        Hidden layer 1 takes actual input data, add it's weight\n",
    "        and biases, and send the output to second layer and so on. \n",
    "        Output of hidden layer 3 will be fed to output layer \n",
    "\t\trelu - Activation funtion, Computes rectified linear: max(features, 0)\n",
    "    \"\"\"\n",
    "    output_hl1 = tf.add(tf.matmul(data,hidden_layer_1['weights']),hidden_layer_1['biases'])\n",
    "    output_hl1 = tf.nn.relu(output_hl1)\n",
    "    \n",
    "    output_hl2 = tf.add(tf.matmul(output_hl1,hidden_layer_2['weights']),hidden_layer_2['biases'])\n",
    "    output_hl2 = tf.nn.relu(output_hl2)\n",
    "    \n",
    "    output_hl3 = tf.add(tf.matmul(output_hl2,hidden_layer_3['weights']),hidden_layer_3['biases'])\n",
    "    output_hl3 = tf.nn.relu(output_hl3)\n",
    "    \n",
    "    \"\"\" For each neuron in output layer:\n",
    "        output = (input*weight) + biases\"\"\"\n",
    "    output = tf.add(tf.matmul(output_hl3,output_layer['weights']),output_layer['biases'])\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-45-a6842d3c8186>:7 in train_neural_netwrok.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 0 completed out of 10, with epoch_loss 2229932.7254\n",
      "Epoch 1 completed out of 10, with epoch_loss 442999.648571\n",
      "Epoch 2 completed out of 10, with epoch_loss 237133.868792\n",
      "Epoch 3 completed out of 10, with epoch_loss 141925.425902\n",
      "Epoch 4 completed out of 10, with epoch_loss 87421.2980564\n",
      "Epoch 5 completed out of 10, with epoch_loss 53707.4115194\n",
      "Epoch 6 completed out of 10, with epoch_loss 37381.1915681\n",
      "Epoch 7 completed out of 10, with epoch_loss 27981.0554463\n",
      "Epoch 8 completed out of 10, with epoch_loss 20980.2657408\n",
      "Epoch 9 completed out of 10, with epoch_loss 20402.8236441\n",
      "Accuracy : 0.9503\n"
     ]
    }
   ],
   "source": [
    "def train_neural_netwrok(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \"\"\" Training the netwrok to optimize the weights \"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x,epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _,c = sess.run([optimizer,cost], feed_dict={x:epoch_x,y:epoch_y})\n",
    "                epoch_loss += c\n",
    "            print \"Epoch %s completed out of %s, with epoch_loss %s\" %(epoch,num_epochs,epoch_loss)\n",
    "        \n",
    "        \"\"\" \n",
    "        After optimizing weights, run them through our model,\n",
    "        and compare the prediction to actual label, and evaluate the\n",
    "        accuracy of all the test data\n",
    "        \"\"\"\n",
    "        correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        print \"Accuracy : %s\" %(accuracy.eval({x:mnist.test.images,y:mnist.test.labels}))\n",
    "\n",
    "train_neural_netwrok(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
